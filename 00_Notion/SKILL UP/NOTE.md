![[00_Notion/ナレッジメモ帳/src/Untitled 3.png|Untitled 3.png]]

  

Attentionの仕組み解説動画  
[https://gigazine.net/news/20240416-visualizing-transformer-attention/?s=09](https://gigazine.net/news/20240416-visualizing-transformer-attention/?s=09)

  

解釈可能なML

[https://speakerdeck.com/yuyay/jsai2023-tiyutoriaru-jie-shi-ke-neng-naji-jie-xue-xi-shuo-ming-haren-notameka](https://speakerdeck.com/yuyay/jsai2023-tiyutoriaru-jie-shi-ke-neng-naji-jie-xue-xi-shuo-ming-haren-notameka)

  

## RAGとファインチューニングの違い

- ファインチューニング
    
    - LLMに対して追加学習を行う
    
    - パラメータ自体を更新する
    
    - 高精度だが、コストや期間がかかる
    

- RAG
    
    - LLMが外部DBから情報をリアルタイムで取得し、参照して応答を生成
    
    - 再学習不要、更新が容易、ハードルが低い
    

モデルの比較

[[GPT-5-Gemini-Claude-Grok 徹底比較]]